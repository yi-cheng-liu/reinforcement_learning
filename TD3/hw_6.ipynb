{"cells":[{"cell_type":"markdown","metadata":{"id":"Ocfmg_VCJ5fS","nbgrader":{"grade":false,"grade_id":"problem_description","locked":true,"schema_version":1,"solution":false}},"source":["# Homework 6\n","\n","## Bipedal Walker Problem - Twin Delayed DDPG (TD3)\n","\n","In this assignment, we will solve the \"BipedalWalker-v3\" task from the OpenAI Gym using Twin Delayed DDPG (TD3) algorithm.\n","\n","### Problem Desciption\n","\n","In this environment, a 2D bipedal walker has to learn a policy to walk without falling over. The total reward calculation is based on the total distance travelled by the agent. The episode ends when the walker touches the ground or it reaches the far right side of the environment.\n","\n","![image.png](data:image/png;base64,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)\n","\n","\n","### Formulation\n","\n","- *State* $s$:\n","The state consists of 24 observations as follows:\n","\n","Num   | Observation                |  Min   |   Max  | Mean\n","------|----------------------------|--------|--------|------   \n","0     | hull_angle                 |  0     | 2$\\pi$ |  0.5\n","1     | hull_angularVelocity       |  -inf  |  +inf  |  -\n","2     | vel_x                      |  -1    |  +1    |  -\n","3     |  vel_y                     |  -1    |  +1    |  -\n","4     | hip_joint_1_angle          |  -inf  |  +inf  |  -\n","5     | hip_joint_1_speed          |  -inf  |  +inf  |  -\n","6     | knee_joint_1_angle         |  -inf  |  +inf  |  -\n","7     | knee_joint_1_speed         |  -inf  |  +inf  |  -\n","8     | leg_1_ground_contact_flag  |  0     |  1     |  -\n","9     | hip_joint_2_angle          |  -inf  |  +inf  |  -\n","10    | hip_joint_2_speed          |  -inf  |  +inf  |  -\n","11    | knee_joint_2_angle         |  -inf  |  +inf  |  -\n","12    | knee_joint_2_speed         |  -inf  |  +inf  |  -\n","13    | leg_2_ground_contact_flag  |  0     |  1     |  -\n","14-23 | 10 lidar readings          |  -inf  |  +inf  |  -\n","\n","\n","- *Action $a$*:\n","The continuous action has four dimensions:\n","\n","Num | Name                        | Min  | Max  \n","----|-----------------------------|------|------\n","0   | Hip_1 (Torque / Velocity)   |  -1  | +1\n","1   | Knee_1 (Torque / Velocity)  |  -1  | +1\n","2   | Hip_2 (Torque / Velocity)   |  -1  | +1\n","3   | Knee_2 (Torque / Velocity)  |  -1  | +1\n","\n","\n","- *Reward $r(s,a)$*:\n","    \n","    Reward is given for moving forward, the walker can get 300+ points when reaching the far end. If the walker falls, it gets -100.\n","    \n","\n","- Episode Termination\n","\n","    An episode terminates when one of the following occurs:\n","  - The walker reaches the far right side of the environment.\n","  - The walker touches the ground.\n","  - Episode length is greater than 1600.\n","\n","\n","- *Objective*:\n","    \n","    To get an average reward of more than 270 over 50 consecutive trials (episodes).\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jerry/Desktop/Github/reinforcement_learning/rl_env/bin/python\n"]}],"source":["import sys\n","print(sys.executable)"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"j9CpC88gJ5fU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: swig in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (4.2.1)\n","Requirement already satisfied: gym[box2d] in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (from gym[box2d]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (from gym[box2d]) (3.0.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (from gym[box2d]) (0.0.8)\n","Requirement already satisfied: box2d-py==2.3.5 in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (from gym[box2d]) (2.3.5)\n","Requirement already satisfied: pygame==2.1.0 in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (from gym[box2d]) (2.1.0)\n","Requirement already satisfied: swig==4.* in /home/jerry/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages (from gym[box2d]) (4.2.1)\n"]}],"source":["# Import packages. Run this cell.\n","!pip install swig\n","!pip install gym[box2d]\n","import numpy as np\n","import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import deque\n","import random\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"ywzptno4J5fV","nbgrader":{"grade":false,"grade_id":"questions","locked":true,"schema_version":1,"solution":false}},"source":["### Task\n","(50 points)\n","\n","We will use the Twin Delayed DDPG (TD3) algorithm to solve the \"BipedalWalker-v3\" task.\n","\n","Please complete the function ``train(self, cur_time_step, episode_time_step, state, action, reward, next_state, done)`` in the class ``TD3``. ``train(self, cur_time_step, episode_time_step, state, action, reward, next_state, done)`` will be called each step the agent interacts with the environment in the code provided for your training. The function collects samples and updates actor network and critic network using mini-batches of experience tuples. The input arguments are as follows:\n","- cur_time_step: current time step counting from the beginning, which is equal to the number of times the agent interacts with the environment\n","- episode_time_step: the time step counting from the current episode\n","- state: current state, a numpy array with shape (state_size,)\n","- action: current action, a numpy array with shape (action_size,)\n","- reward: reward obtained\n","- next_state: next state, a numpy array with shape (state_size,)\n","- done: ``True`` when the current episode ends, ``False`` otherwise\n","   \n","You can also add or revise classes and functions if you need.\n","\n","After you complete the function, run the code to train your actor and critic networks, and save your actor network as ``actor.pth`` (save the model's ``state_dict``). Then upload ``actor.pth`` to Gradescope. We will test your policy (actor) for 50 episodes on Gradescope.\n","\n","**Note**:\n","- You must use the class ``Actor`` as your actor network. Do not modify the structure of the actor network. Do not modify any code in the class ``Actor``.\n","- You can use Google Colab to do the training.\n","- You can use the class ``CriticQ`` as your critic network.\n","- You can use GPU to accelerate the training, but make sure that the actor network you saved is on CPU.\n","\n","**Recommended Hyperparameters**:\n","- Learning rate for the actor network: 1e-3\n","- Learning rate for the critic network: 1e-3\n","- Replay buffer capacity: 100000\n","- Batch size: 128\n","- Soft update step size for target networks: $\\tau$=0.02\n","\n","    Target networks are updated towards main networks according to:\n","    \\begin{align}\n","        \\theta_{\\text{targ}} \\leftarrow (1-\\tau) \\theta_{\\text{targ}} + \\tau \\theta\n","    \\end{align}\n","\n","- Policy update period: 2\n","\n","    Policy will be updated once every 2 updates of the Q-networks.\n","    \n","- Discount factor: 0.99\n","- Standard deviation for smoothing noise added to target policy: 0.2\n","- Limit for absolute value of target policy smoothing noise: 0.5\n","- Number of environment interactions that should elapse between gradient descent updates: 200\n","\n","    Note: Regardless of how long you wait between updates, the ratio of env steps to gradient steps is 1. For example, we update once every 200 environment interations and each update includes 200 gradient steps for Q-networks.\n"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"gv_g8egpJ5fW","nbgrader":{"grade":false,"grade_id":"question_code_networks","locked":true,"schema_version":1,"solution":false}},"outputs":[],"source":["\"\"\"\n","Some parameters\n","\"\"\"\n","state_size = 24  # state dimension\n","action_size = 4  # action dimension\n","fc_units = 256  # number of neurons in one fully connected hidden layer\n","action_upper_bound = 1  # action space upper bound\n","action_lower_bound = -1  # action space lower bound\n","\n","\n","\"\"\"\n","Structure of Actor Network.\n","\"\"\"\n","class Actor(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.max_action = action_upper_bound\n","        self.fc1 = nn.Linear(state_size, fc_units)\n","        self.fc2 = nn.Linear(fc_units, fc_units)\n","        self.fc3 = nn.Linear(fc_units, action_size)\n","\n","    def forward(self, state):\n","        \"\"\"\n","        Build an actor (policy) network that maps states -> actions.\n","        Args:\n","            state: torch.Tensor with shape (batch_size, state_size)\n","        Returns:\n","            action: torch.Tensor with shape (batch_size, action_size)\n","        \"\"\"\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        action = torch.tanh(self.fc3(x)) * self.max_action\n","        return action\n","\n","\n","\"\"\"\n","Structure of Critic Network.\n","\"\"\"\n","class CriticQ(nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        Args:\n","            state_size: state dimension\n","            action_size: action dimension\n","            fc_units: number of neurons in one fully connected hidden layer\n","        \"\"\"\n","        super().__init__()\n","\n","        # Q-network 1 architecture\n","        self.l1 = nn.Linear(state_size + action_size, fc_units)\n","        self.l2 = nn.Linear(fc_units, fc_units)\n","        self.l3 = nn.Linear(fc_units, 1)\n","\n","        # Q-network 2 architecture\n","        self.l4 = nn.Linear(state_size + action_size, fc_units)\n","        self.l5 = nn.Linear(fc_units, fc_units)\n","        self.l6 = nn.Linear(fc_units, 1)\n","\n","    def forward(self, state, action):\n","        \"\"\"\n","        Build a critic (value) network that maps state-action pairs -> Q-values.\n","        Args:\n","            state: torch.Tensor with shape (batch_size, state_size)\n","            action: torch.Tensor with shape (batch_size, action_size)\n","        Returns:\n","            Q_value_1: torch.Tensor with shape (batch_size, 1)\n","            Q_value_2: torch.Tensor with shape (batch_size, 1)\n","        \"\"\"\n","        state_action = torch.cat([state, action], 1)\n","\n","        x1 = F.relu(self.l1(state_action))\n","        x1 = F.relu(self.l2(x1))\n","        Q_value_1 = self.l3(x1)\n","\n","        x2 = F.relu(self.l4(state_action))\n","        x2 = F.relu(self.l5(x2))\n","        Q_value_2 = self.l6(x2)\n","\n","        return Q_value_1, Q_value_2"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"N9ds4SfvJ5fW","nbgrader":{"grade":false,"grade_id":"question_code","locked":false,"schema_version":1,"solution":true}},"outputs":[],"source":["\"\"\"\n","Implementation of TD3 Algorithm\n","\"\"\"\n","class TD3:\n","    def __init__(self):\n","        self.lr_actor = 1e-3   # learning rate for actor network\n","        self.lr_critic = 1e-3  # learning rate for critic network\n","        self.buffer_capacity = 100000  # replay buffer capacity\n","        self.batch_size = 128  # mini-batch size\n","        self.tau = 0.02  # soft update parameter\n","        self.policy_delay = 2  # policy will be updated once every policy_delay times for each update of the Q-networks.\n","        self.gamma = 0.99  # discount factor\n","        self.target_noise = 0.2  # standard deviation for smoothing noise added to target policy\n","        self.noise_clip = 0.5  # limit for absolute value of target policy smoothing noise.\n","        self.update_every = 200  # number of env interactions that should elapse between updates of Q-networks.\n","        # Note: Regardless of how long you wait between updates, the ratio of env steps to gradient steps should be 1.\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.device = torch.device(\"cpu\")  # or self.device = torch.device(\"cuda\")\n","        self.action_upper_bound = action_upper_bound  # action space upper bound\n","        self.action_lower_bound = action_lower_bound  # action space lower bound\n","        self.create_actor()\n","        self.create_critic()\n","        self.act_opt = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n","        self.crt_opt = optim.Adam(self.critic.parameters(), lr=self.lr_critic)\n","        self.replay_memory_buffer = deque(maxlen=self.buffer_capacity)\n","\n","    def create_actor(self):\n","        self.actor = Actor().to(self.device)\n","        self.actor_target = Actor().to(self.device)\n","        self.actor_target.load_state_dict(self.actor.state_dict())\n","\n","    def create_critic(self):\n","        self.critic = CriticQ().to(self.device)\n","        self.critic_target = CriticQ().to(self.device)\n","        self.critic_target.load_state_dict(self.critic.state_dict())\n","\n","    def add_to_replay_memory(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Add samples to replay memory\n","        Args:\n","            state: current state, a numpy array with shape (state_size,)\n","            action: current action, a numpy array with shape (action_size,)\n","            reward: reward obtained\n","            next_state: next state, a numpy array with shape (state_size,)\n","            done: True when the current episode ends, False otherwise\n","        \"\"\"\n","        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n","\n","    def get_random_sample_from_replay_mem(self):\n","        \"\"\"\n","        Random samples from replay memory without replacement\n","        Returns a self.batch_size length list of unique elements chosen from the replay buffer.\n","        Returns:\n","            random_sample: a list with len=self.batch_size,\n","                           where each element is a tuple (state, action, reward, next_state, done)\n","        \"\"\"\n","        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n","        return random_sample\n","\n","    def soft_update_target(self, local_model, target_model):\n","        \"\"\"\n","        Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Args:\n","            local_model: PyTorch model (weights will be copied from)\n","            target_model: PyTorch model (weights will be copied to)\n","            tau (float): interpolation parameter\n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n","\n","    def train(self, cur_time_step, episode_time_step, state, action, reward, next_state, done):\n","        \"\"\"\n","        Collect samples and update actor network and critic network using mini-batches of experience tuples.\n","        Args:\n","            cur_time_step: current time step counting from the beginning,\n","                           which is equal to the number of times the agent interacts with the environment\n","            episode_time_step: the time step counting from the current episode\n","            state: current state, a numpy array with shape (state_size,)\n","            action: current action, a numpy array with shape (action_size,)\n","            reward: reward obtained\n","            next_state: next state, a numpy array with shape (state_size,)\n","            done: True when the current episode ends, False otherwise\n","        \"\"\"\n","        self.add_to_replay_memory(state, action, reward, next_state, done)\n","        if len(self.replay_memory_buffer) < self.batch_size:\n","            return\n","        if cur_time_step % self.update_every != 0:\n","            return\n","\n","        # Perform self.update_every times of updates of the critic networks and\n","        # (self.update_every / policy_delay) times of updates of the actor network\n","        for it in range(self.update_every):\n","            \"\"\"\n","            state_batch: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of current states\n","            action_batch: torch.Tensor with shape (self.batch_size, action_size), a mini-batch of current actions\n","            reward_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards\n","            next_state_batch: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of next states\n","            done_list: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers,\n","                   where 1 means the episode terminates for that sample;\n","                         0 means the episode does not terminate for that sample.\n","            \"\"\"\n","            mini_batch = self.get_random_sample_from_replay_mem()\n","            state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float().to(self.device)\n","            action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).float().to(self.device)\n","            reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float().to(self.device)\n","            next_state_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float().to(self.device)\n","            done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float().to(self.device)\n","\n","            # Please complete codes for updating the critic networks\n","            \"\"\"\n","            Hints:\n","              You may use the above tensors: state_batch, action_batch, reward_batch, next_state_batch, done_list\n","              You may use self.critic_target and self.actor_target as your target networks\n","              you may use target policy smoothing techniques with hyperparameters self.target_noise and self.noise_clip\n","              You may use clipped double Q-learning techniques\n","              You may update self.critic using the optimizer self.crt_opt and MSE loss function.\n","              Make sure to consider whether the corresponding episode terminates when calculating target values.\n","                If the episode terminates, then the next state value should be 0.\n","            \"\"\"\n","            ### BEGIN SOLUTION\n","            # YOUR CODE HERE\n","            with torch.no_grad():\n","                next_action = self.actor_target(next_state_batch)\n","                noise = torch.clamp(torch.randn_like(next_action) * self.target_noise, -self.noise_clip, self.noise_clip)\n","                next_action = torch.clamp(next_action + noise, self.action_lower_bound, self.action_upper_bound)\n","                target_Q1, target_Q2 = self.critic_target(next_state_batch, next_action)\n","                target_Q = torch.min(target_Q1, target_Q2)\n","                target_Q = reward_batch + (1 - done_list) * self.gamma * target_Q\n","            \n","            # Update Critic\n","            current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n","            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","            self.crt_opt.zero_grad()\n","            critic_loss.backward()\n","            self.crt_opt.step()\n","            \n","            ### END SOLUTION\n","\n","            # Train Actor\n","            # Delayed policy updates\n","            # Update self.actor once every policy_delay times for each update of self.critic\n","            if it % self.policy_delay == 0:\n","\n","                # Please complete codes for updating of the actor network\n","                \"\"\"\n","                Hint:\n","                  You may update self.actor using the optimizer self.act_opt and recall the loss function for DDPG training\n","                \"\"\"\n","                ### BEGIN SOLUTION\n","                # YOUR CODE HERE\n","                \n","                # Update policy by one step of gradient ascent using the deterministic policy gradient\n","                actor_loss = -self.critic.Q1(state_batch, self.actor(state_batch)).mean()\n","                self.act_opt.zero_grad()\n","                actor_loss.backward()\n","                self.act_opt.step()\n","                \n","                \n","                ### END SOLUTION\n","\n","                # Soft update target models\n","                self.soft_update_target(self.critic, self.critic_target)\n","                self.soft_update_target(self.actor, self.actor_target)\n","\n","\n","    def policy(self, state):\n","        \"\"\"\n","        Select action based on the actor network.\n","        Args:\n","            state: a numpy array with shape (state_size,)\n","        Returns:\n","            actions: a numpy array with shape (action_size,)\n","        \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        self.actor.eval()\n","        with torch.no_grad():\n","            actions = np.squeeze(self.actor(state).cpu().data.numpy())\n","        self.actor.train()\n","        return actions\n"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"RGcS7YsMJ5fY","nbgrader":{"grade":true,"grade_id":"question_test_1","locked":true,"points":3,"schema_version":1,"solution":false}},"outputs":[{"ename":"AttributeError","evalue":"'BipedalWalker' object has no attribute 'seed'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBipedalWalker-v3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 5\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m~/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Github/reinforcement_learning/rl_env/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mAttributeError\u001b[0m: 'BipedalWalker' object has no attribute 'seed'"]}],"source":["# The following code is provided for the training of your agent in the 'BipedalWalker-v3' gym environment.\n","gym.logger.set_level(40)\n","env = gym.make('BipedalWalker-v3')\n","_ = env.reset()\n","env.seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","timesteps_count = 0  # Counting the time steps\n","max_steps = 1600  # Maximum time steps for one episode\n","ep_reward_list = deque(maxlen=50)\n","avg_reward = -9999\n","agent = TD3()\n","\n","for ep in range(600):\n","    state = env.reset()\n","    episodic_reward = 0\n","    timestep_for_cur_episode = 0\n","\n","    for st in range(max_steps):\n","        # Select action according to policy\n","        action = agent.policy(state)\n","\n","        # Recieve state and reward from environment.\n","        next_state, reward, done, info = env.step(action)\n","        episodic_reward += reward\n","\n","        # Send the experience to the agent and train the agent\n","        agent.train(timesteps_count, timestep_for_cur_episode, state, action, reward, next_state, done)\n","\n","        timestep_for_cur_episode += 1\n","        timesteps_count += 1\n","\n","        # End this episode when `done` is True\n","        if done:\n","            break\n","        state = next_state\n","\n","    ep_reward_list.append(episodic_reward)\n","    print('Ep. {}, Ep.Timesteps {}, Episode Reward: {:.2f}'.format(ep + 1, timestep_for_cur_episode, episodic_reward), end='')\n","\n","    if len(ep_reward_list) == 50:\n","        # Mean of last 50 episodes\n","        avg_reward = sum(ep_reward_list) / 50\n","        print(', Moving Average Reward: {:.2f}'.format(avg_reward))\n","    else:\n","        print('')\n","\n","print('Average reward over 50 episodes: ', avg_reward)\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RosAJ5m1UvyA"},"outputs":[{"ename":"NameError","evalue":"name 'agent' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the actor\u001b[39;00m\n\u001b[1;32m      2\u001b[0m actor_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstate_dict(), actor_path)\n","\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"]}],"source":["# Save the actor\n","actor_path = \"actor.pth\"\n","torch.save(agent.actor.to(\"cpu\").state_dict(), actor_path)"]}],"metadata":{"accelerator":"GPU","celltoolbar":"Create Assignment","colab":{"provenance":[]},"kernelspec":{"display_name":"rl_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
