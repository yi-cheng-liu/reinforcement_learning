{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMIepUIVYH97",
    "nbgrader": {
     "grade": false,
     "grade_id": "problem_description",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 7 Part 2\n",
    "\n",
    "## Play Othello Using AlphaZero Algorithm\n",
    "\n",
    "In this assignment, we will play the game of Othello using AlphaZero algorithm.\n",
    "\n",
    "### Problem Desciption\n",
    "\n",
    "Othello is a strategy board game for two players (Black and White). The game traditionally begins with four discs placed in the middle of the board as shown below. Black moves first.\n",
    "\n",
    "(Black: O, White: X, No disc: $-$)\n",
    "\n",
    "\n",
    "\\begin{array}{cccc}\n",
    "    -&-&-&-\\\\\n",
    "    -&\\mathrm{X}&\\mathrm{O}&-\\\\\n",
    "    -&\\mathrm{O}&\\mathrm{X}&-\\\\\n",
    "    -&-&-&-\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Black must place a black disc on the board, in such a way that there is at least one straight (horizontal, vertical, or diagonal) occupied line between the new disc and another black disc, with one or more contiguous white pieces between them. In the starting position, Black has the following 4 options indicated by $\\hat{\\mathrm{O}}$:\n",
    "\n",
    "\\begin{array}{cccc}\n",
    "    -                &\\hat{\\mathrm{O}} &-                &-\\\\\n",
    "    \\hat{\\mathrm{O}} &\\mathrm{X}       &\\mathrm{O}       &-\\\\\n",
    "    -                &\\mathrm{O}       &\\mathrm{X}       &\\hat{\\mathrm{O}}\\\\\n",
    "    -                &-                &\\hat{\\mathrm{O}} &-\n",
    "\\end{array}\n",
    "\n",
    "After placing the disc, Black flips all white discs lying on a straight line between the new disc and any existing black discs. All flipped discs are now black. If Black decides to place a disc in the topmost location, one white disc gets flipped, and the board now looks like this:\n",
    "\n",
    "\\begin{array}{cccc}\n",
    "    -                &\\mathrm{O}       &-                &-\\\\\n",
    "    -                &\\mathrm{O}       &\\mathrm{O}       &-\\\\\n",
    "    -                &\\mathrm{O}       &\\mathrm{X}       &-\\\\\n",
    "    -                &-                &-                &-\n",
    "\\end{array}\n",
    "\n",
    "Now White plays. This player operates under the same rules, with the roles reversed: White lays down a white disc, causing black discs to flip. At this time, White has the following 4 options indicated by $\\hat{\\mathrm{X}}$:\n",
    "\n",
    "\\begin{array}{cccc}\n",
    "    \\hat{\\mathrm{X}} &\\mathrm{O}       &\\hat{\\mathrm{X}} &-\\\\\n",
    "    -                &\\mathrm{O}       &\\mathrm{O}       &-\\\\\n",
    "    \\hat{\\mathrm{X}} &\\mathrm{O}       &\\mathrm{X}       &-\\\\\n",
    "    -                &-                &-                &-\n",
    "\\end{array}\n",
    "\n",
    "If White plays the bottom left option and flips one disc:\n",
    "\n",
    "\\begin{array}{cccc}\n",
    "    -                &\\mathrm{O}       &-                &-\\\\\n",
    "    -                &\\mathrm{O}       &\\mathrm{O}       &-\\\\\n",
    "    \\mathrm{X}       &\\mathrm{X}       &\\mathrm{X}       &-\\\\\n",
    "    -                &-                &-                &-\n",
    "\\end{array}\n",
    "\n",
    "Players alternate taking turns. If a player does not have any valid moves, play passes back to the other player. When neither player can move, the game ends. A game of Othello may end before the board is completely filled.\n",
    "\n",
    "The player with the most discs on the board at the end of the game wins. If both players have the same number of discs, then the game is a draw.\n",
    "\n",
    "Reference: The Othello game rules are from https://www.eothello.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eLdbRI7_ZSXc"
   },
   "outputs": [],
   "source": [
    "# Upload the file \"Othello.zip\" and then unzip it\n",
    "try:\n",
    "    from google.colab import files\n",
    "    from zipfile import ZipFile\n",
    "    uploaded = files.upload()\n",
    "    with ZipFile(\"Othello.zip\", 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7uQRNi6sYH9-"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Arena'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mArena\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Arena\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOthelloGame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OthelloGame\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOthelloPlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Arena'"
     ]
    }
   ],
   "source": [
    "# Import packages. Run this cell.\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from Arena import Arena\n",
    "from OthelloGame import OthelloGame\n",
    "from OthelloPlayers import *\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "# The environment \"Arena\", \"OthelloGame\", \"OthelloPlayers\" \n",
    "# is modified from https://github.com/suragnair/alpha-zero-general\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qo8Faj4FYH9_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OthelloGame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You can use this cell to play the game with a random player or a greedy player\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Be sure to remove/comment these codes before submitting the assignment\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m game \u001b[38;5;241m=\u001b[39m \u001b[43mOthelloGame\u001b[49m(\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# An Othello game with a 4*4 board\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Players\u001b[39;00m\n\u001b[1;32m      7\u001b[0m random_player \u001b[38;5;241m=\u001b[39m RandomPlayer(game)\u001b[38;5;241m.\u001b[39mplay\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OthelloGame' is not defined"
     ]
    }
   ],
   "source": [
    "# You can use this cell to play the game with a random player or a greedy player\n",
    "# Be sure to remove/comment these codes before submitting the assignment\n",
    "\n",
    "game = OthelloGame(4)  # An Othello game with a 4*4 board\n",
    "\n",
    "# Players\n",
    "random_player = RandomPlayer(game).play\n",
    "greedy_player = GreedyOthelloPlayer(game).play\n",
    "human_player = HumanOthelloPlayer(game).play\n",
    "\n",
    "arena = Arena(human_player, greedy_player, game, display=OthelloGame.display)\n",
    "_ = arena.playGame(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kXnDPRVYH-A",
    "nbgrader": {
     "grade": false,
     "grade_id": "question",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Task\n",
    "(50 points)\n",
    "\n",
    "We will implement the AlphaZero algorithm to play the Othello game with size $5\\times 5$. Let $n \\times n$ be the size of the board. In this case, $n=5$.\n",
    "\n",
    "Please complete the class ``Coach()`` and the class ``MCTS()``. For testing, we will first initialize a class object ``coach = Coach(game)``, and then call the function ``coach.train()``. Next, we will pit the trained agent against a random player to evaluate the performance.\n",
    "\n",
    "You can also write your own codes,\n",
    "but completing the existing codes will be easier than starting from scratch.\n",
    "\n",
    "Note that the platform does not support GPU, so please use CPU.\n",
    " \n",
    "\n",
    "**Recommended Hyperparameters**:\n",
    "\n",
    " - You can use the ``PolicyNet`` class for your policy network.\n",
    " - Optimizer: Adam\n",
    " - Learning rate: 0.001\n",
    " - Batch size: 64\n",
    " - Number of training epochs for each iteration: 10\n",
    " - Number of simulations for MCTS for each action: 50\n",
    " - Coefficient for UCB bonus term for MCTS: 1.0\n",
    " - Number of iterations: 2\n",
    " - Number of complete self-play games for one iteration: 20\n",
    " - Number of games to play during arena play to determine if new net will be accepted: 40\n",
    " - During arena playoff, new neural net will be accepted if threshold or more of games are won: 0.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEyS_qVMYH-A",
    "nbgrader": {
     "grade": false,
     "grade_id": "env_functions",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Functions for your reference\n",
    "In this section, we list some functions in the environment codes for your reference.\n",
    "\n",
    "\n",
    "The class ``Arena``:\n",
    "\n",
    " - ``def __init__(self, player1, player2, game, display=None):``\n",
    " \n",
    "    Args:\n",
    "        player1, player2: two functions that takes board as input, return action. For example, coach.play.\n",
    "        game: OthelloGame object\n",
    "        display: a function that takes board as input and prints it. It can be set to OthelloGame.display.\n",
    "\n",
    "\n",
    " - ``def playGame(self, verbose=False):`` Play one game. Return 1 if player1 won the game, -1 if player2 won the game, 0.000001 if it is a draw.\n",
    "\n",
    "\n",
    " - ``def playGames(self, num, verbose=False):`` Play ``num`` games.\n",
    " \n",
    "    Returns: (oneWon, twoWon, draws)\n",
    "        oneWon: number of games won by player1\n",
    "        twoWon: number of games won by player2\n",
    "        draws:  number of games won by nobody\n",
    "\n",
    "\n",
    "The class ``OthelloGame``:\n",
    "\n",
    " - ``def getBoardSize(self):`` Returns a tuple ``(n, n)``, the board size of the game.\n",
    " \n",
    " \n",
    " - ``def getActionSize(self):`` Retruns the number of actions, including the pass action. The number of actions is n*n+1.\n",
    " \n",
    " \n",
    " - ``def getCanonicalForm(self, board, player):``\n",
    "     \n",
    "     Args:\n",
    "         board: board configuration, a 2D numpy array:\n",
    "                1=black, -1=white, 0=empty\n",
    "                first dim is row , second is column\n",
    "         player: 1=Black player, -1=White player\n",
    "     Return state if player==1 (Black player), else return -state if player==-1 (White player)\n",
    " \n",
    " \n",
    " - ``def stringRepresentation(self, board):``\n",
    "     \n",
    "     Args:\n",
    "         board: board configuration, a 2D numpy array:\n",
    "                1=black, -1=white, 0=empty\n",
    "                first dim is row , second is column\n",
    "     Return board.tostring()\n",
    " \n",
    " \n",
    " - ``def getGameEnded(self, board, player):``\n",
    " \n",
    "     Args:\n",
    "         board: board configuration, a 2D numpy array:\n",
    "                1=black, -1=white, 0=empty\n",
    "                first dim is row , second is column\n",
    "         player: 1=Black player, -1=White player\n",
    "     Return 0 if not ended, 1 if the player won, -1 if the player lost, 0.000001 if it is a draw.\n",
    "     \n",
    "\n",
    " - ``def getValidMoves(self, board, player)``\n",
    " \n",
    "     Args:\n",
    "         board: board configuration, a 2D numpy array:\n",
    "                1=black, -1=white, 0=empty\n",
    "                first dim is row , second is column\n",
    "         player: 1=Black player, -1=White player\n",
    "     Return the valid moves, a binary numpy array with shape (action_size,), where the ith element=1 means that action i is valid and 0 means that action i is invalid.\n",
    "         \n",
    "     Note: Let the board size be n\\*n. Putting a disc on row x and column y of the board corresponds to action=x\\*n+y. action=n\\*n means passing. (Row and column are counting from 0 to n-1.)\n",
    "         \n",
    "     \n",
    " - ``def game.getNextState(self, board, player, action)``\n",
    " \n",
    "     Args:\n",
    "         board: board configuration, a 2D numpy array:\n",
    "                1=black, -1=white, 0=empty\n",
    "                first dim is row , second is column\n",
    "         player: 1=Black player, -1=White player\n",
    "         action: Putting a disc on row x and column y of the board corresponds to action=x\\*n+y. action=n\\*n means passing. (Row and column are counting from 0 to n-1.) Note that action must be a valid move.\n",
    "     Return next (board, player)\n",
    "     \n",
    "\n",
    " - ``def getInitBoard(self):``\n",
    " \n",
    "     Return initial board configuration, a 2D numpy array.\n",
    " \n",
    " \n",
    " - ``def getSymmetries(self, board, pi):``\n",
    "     \n",
    "     Args:\n",
    "         board: board configuration, a 2D numpy array:\n",
    "                1=black, -1=white, 0=empty\n",
    "                first dim is row , second is column\n",
    "         pi: a policy vector, a list with len=action_size\n",
    "     Return a list of (board, pi). These are 8 symmetic board configurations and the corresponding policy vectors.\n",
    "     \n",
    "     This can be used to increase the number of training samples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zJsPrSQ6YH-B",
    "nbgrader": {
     "grade": false,
     "grade_id": "codes_policy_net",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the policy network \n",
    "    \"\"\"\n",
    "    def __init__(self, game):\n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.num_channels = 256  # number of channels for the Conv2d layer\n",
    "        self.dropout = 0.3  # Dropout probability\n",
    "        \n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, self.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(self.num_channels, self.num_channels, 3, stride=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(self.num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.num_channels*(self.board_x-2)*(self.board_y-2), 512)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, self.action_size)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: board configurtion, torch.Tensor with shape (batch_size, board_x, board_y)\n",
    "        Returns:\n",
    "            pi: log probability of actions in state s, torch.Tensor with shape (batch_size, action_size)\n",
    "            v: value of state s, torch.Tensor with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "        s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "        s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "        s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "        s = s.view(-1, self.num_channels*(self.board_x-2)*(self.board_y-2))\n",
    "\n",
    "        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "        # log probability of actions in state s\n",
    "        pi = F.log_softmax(self.fc2(s), dim=1)                                                   # batch_size x action_size\n",
    "        # value of state s\n",
    "        v = torch.tanh(self.fc3(s))                                                              # batch_size x 1\n",
    "\n",
    "        return pi, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ro4OLnIHYH-C",
    "nbgrader": {
     "grade": false,
     "grade_id": "codes_MCTS",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# The following is a class to implement MCTS.\n",
    "# You can also write your own codes to implement MCTS,\n",
    "# but completing the following codes will be easier than starting from scratch.\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, game, policy_net):\n",
    "        self.game = game\n",
    "        self.policy_net = policy_net\n",
    "        \n",
    "        self.num_MCTS_sims = 50  # number of simulations for MCTS for each action\n",
    "        self.bonus_term_factor = 1.0\n",
    "        \n",
    "        self.Qsa = {}  # stores Q values for s,a\n",
    "        self.Nsa = {}  # stores number of times edge s,a was visited\n",
    "        self.Ns = {}  # stores number of times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by policy network)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs num_MCTS_sims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        \n",
    "        Args:\n",
    "            canonicalBoard: canonical board configuration, a 2D numpy array:\n",
    "                            1=current player, -1=the opponent, 0=empty\n",
    "                            first dim is row , second is column\n",
    "        Returns:\n",
    "            probs: a list with len=action_size, which is a policy vector \n",
    "                   where the probability of the ith action is proportional to Nsa[(s,a)]\n",
    "        \"\"\"\n",
    "        # Doing self.num_MCTS_sims times of simulations starting from the state 'canonicalBoard'\n",
    "        for i in range(self.num_MCTS_sims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        # Use string representation for the state\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        \"\"\"\n",
    "        Please complete the codes for calculating the updated policy vector 'probs' using 'self.Nsa'\n",
    "        Some information you may need:\n",
    "            self.Nsa[(s, a)] stores number of times edge s,a was visited.\n",
    "            If (s,a) is not in self.Nsa, then s has not been visited.\n",
    "            self.game.getActionSize() returns the number of actions, i.e., n*n+1.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ### END SOLUTION\n",
    "        return probs\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one simulation of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        \n",
    "        This is a recursive function.\n",
    "        \n",
    "        Args:\n",
    "            canonicalBoard: canonical board configuration, a 2D numpy array:\n",
    "                            1=current player, -1=the opponent, 0=empty\n",
    "                            first dim is row , second is column\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use string representation for the state\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        \n",
    "        # Update self.Es\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        \n",
    "        \n",
    "        if self.Es[s] != 0:  # The game ended, which means that s is a terminal node\n",
    "            # If the current player won, then return -1 (The value for the other player).\n",
    "            # Otherwise, return 1 (The value for the other player).\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:  # There is no policy for the current state s, which means that s is a leaf node (a new state)\n",
    "            \n",
    "            # Set Q(s,a)=0 and N(s,a)=0 for all a\n",
    "            for a in range(self.game.getActionSize()):\n",
    "                self.Qsa[(s, a)] = 0\n",
    "                self.Nsa[(s, a)] = 0\n",
    "            \n",
    "            # Calculate the output of the policy network, which are the policy and the value for state s\n",
    "            board = torch.FloatTensor(canonicalBoard.astype(np.float64)).view(1, self.policy_net.board_x,\n",
    "                                                                              self.policy_net.board_y)\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                pi, v = self.policy_net(board)\n",
    "            self.Ps[s] = torch.exp(pi).data.cpu().numpy()[0]  # The policy for state s\n",
    "            v = v.data.cpu().numpy()[0][0]  # The value of state s\n",
    "            \n",
    "            # Masking invalid moves\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  \n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "            \n",
    "            self.Vs[s] = valids  # Stores the valid moves\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "        \n",
    "        # pick the action with the highest upper confidence bound (ucb) and assign it to best_act\n",
    "        best_act = -1\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                \"\"\"\n",
    "                Please complete the codes for picking the action with the highest UCB\n",
    "                Some information you may need:\n",
    "                    self.Qsa[(s, a)] stores the Q value for s,a\n",
    "                    self.bonus_term_factor=1.0 is the factor \"h\" in the UCB (See Eq.(1) in the reference guide)\n",
    "                    self.Ps stores the policy returned by policy network\n",
    "                    self.Ps[s][a] is the probability corresponding to state s and action a\n",
    "                    self.Ns[s] stores the number of times board s was visited\n",
    "                    self.Nsa[(s, a)] stores number of times edge s,a was visited\n",
    "                \"\"\"\n",
    "                ### BEGIN SOLUTION\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                ### END SOLUTION\n",
    "        \n",
    "        # Continue the simulation: take action best_act in the simulation\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)  # This returns the value for the current player\n",
    "        \n",
    "        \"\"\"\n",
    "        Please complete the codes for updating the Q function ('self.Qsa')\n",
    "        and the number of times that (s,a) has been visited ('self.Nsa')\n",
    "        Some information you may need:\n",
    "            self.Qsa[(s, a)] stores the Q value for s,a\n",
    "            self.Ns[s] stores the number of times board s was visited\n",
    "            self.Nsa[(s, a)] stores number of times edge s,a was visited\n",
    "            v is the value for the current player\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # Update the number of times that s has been visited\n",
    "        self.Ns[s] += 1\n",
    "        \n",
    "        return -v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wj2xKch3YH-D",
    "nbgrader": {
     "grade": false,
     "grade_id": "codes_coach",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# The following is a class to implement the whole learning process.\n",
    "# You can also write your own codes,\n",
    "# but completing the following codes will be easier than starting from scratch.\n",
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.nnet = PolicyNet(game)\n",
    "        self.pnet = PolicyNet(game)  # the competitor network\n",
    "        self.mcts = MCTS(game, self.nnet)\n",
    "        self.epochs = 10  # number of training epochs for each iteration\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 64  # batch size\n",
    "        self.trainExamples = []  # historical examples for training\n",
    "        self.numIters = 2  # number of iterations\n",
    "        self.numEps = 20  # number of complete self-play games for one iteration.\n",
    "        self.arenaCompare = 40  # number of games to play during arena play to determine if new net will be accepted.\n",
    "        self.updateThreshold = 0.6  # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "        for i in range(1, self.numIters + 1):\n",
    "            print(f'Starting Iter #{i} ...')\n",
    "\n",
    "            for _ in tqdm(range(self.numEps), desc=\"Self Play\"):\n",
    "                self.mcts = MCTS(self.game, self.nnet)  # reset search tree\n",
    "                self.trainExamples.extend(self.executeEpisode()) # save the iteration examples to the history\n",
    "            \n",
    "            # shuffle examples before training           \n",
    "            shuffle(self.trainExamples)\n",
    "\n",
    "            # training new network, keeping a copy of the old one\n",
    "            self.pnet.load_state_dict(self.nnet.state_dict())\n",
    "\n",
    "            optimizer = optim.Adam(self.nnet.parameters(), lr=self.learning_rate)\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                print('EPOCH ::: ' + str(epoch + 1))\n",
    "                self.nnet.train()\n",
    "                \n",
    "                \"\"\"\n",
    "                Please complete the training codes for self.nnet\n",
    "                Some information you may need:\n",
    "                    self.trainExamples is a list that stores historical examples for training\n",
    "                    self.trainExamples[i] has the form (canonicalBoard, pi, v)\n",
    "                    The output of self.nnet include pi and v, where\n",
    "                        pi are the log probabilities of actions in state s;\n",
    "                        v is the value of state s.\n",
    "                \"\"\"\n",
    "                ### BEGIN SOLUTION\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                ### END SOLUTION\n",
    "            \n",
    "            pmcts = MCTS(self.game, self.pnet)\n",
    "            nmcts = MCTS(self.game, self.nnet)\n",
    "\n",
    "            print('PITTING AGAINST PREVIOUS VERSION')\n",
    "            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x)),\n",
    "                          lambda x: np.argmax(nmcts.getActionProb(x)), self.game)\n",
    "            pwins, nwins, draws = arena.playGames(self.arenaCompare)\n",
    "\n",
    "            print('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "            if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.updateThreshold:\n",
    "                print('REJECTING NEW MODEL')\n",
    "                self.nnet.load_state_dict(self.pnet.state_dict())\n",
    "            else:\n",
    "                print('ACCEPTING NEW MODEL')\n",
    "                self.pnet.load_state_dict(self.nnet.state_dict())\n",
    "                self.trainExamples = []\n",
    "        \n",
    "    def play(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            canonicalBoard: canonical board configuration, a 2D numpy array:\n",
    "                            1=current player, -1=the opponent, 0=empty\n",
    "                            first dim is row , second is column\n",
    "        Returns:\n",
    "            action: Putting a disc on row x and column y of the board corresponds to action=x*n+y. action=n*n means passing.\n",
    "            (Row and column are counting from 0 to n-1.) \n",
    "        \"\"\"\n",
    "        mcts = MCTS(self.game, self.nnet)\n",
    "        action = np.argmax(mcts.getActionProb(canonicalBoard))\n",
    "        return action\n",
    "    \n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1 (Black player).\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard, pi, v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, -1 if the player lost the game, and otherwise 0.000001\n",
    "        \"\"\"\n",
    "        trainExamples = []\n",
    "        board = self.game.getInitBoard()\n",
    "        self.curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            canonicalBoard = self.game.getCanonicalForm(board, self.curPlayer)\n",
    "            \n",
    "            # After 10 steps, we use the greedy action rather than a random action\n",
    "            if episodeStep < 10:\n",
    "                pi = self.mcts.getActionProb(canonicalBoard)\n",
    "            else:\n",
    "                pi = list(np.zeros((self.game.getActionSize(),)))\n",
    "                pi[np.argmax(self.mcts.getActionProb(canonicalBoard))] = 1\n",
    "            \n",
    "            # Add symmetric samples\n",
    "            sym = self.game.getSymmetries(canonicalBoard, pi)\n",
    "            \n",
    "            for b, p in sym:\n",
    "                trainExamples.append([b, self.curPlayer, p, None])\n",
    "            \n",
    "            # Take action according to the policy pi\n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n",
    "\n",
    "            r = self.game.getGameEnded(board, self.curPlayer)\n",
    "\n",
    "            if r != 0:  # if the current episode of game ended\n",
    "                trainExamples = [(x[0], x[2], r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]\n",
    "                return trainExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WcGASaJxYH-F",
    "nbgrader": {
     "grade": true,
     "grade_id": "test_1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OthelloGame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m game \u001b[38;5;241m=\u001b[39m \u001b[43mOthelloGame\u001b[49m(\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# An Othello game with a 5*5 board\u001b[39;00m\n\u001b[1;32m      7\u001b[0m random_player \u001b[38;5;241m=\u001b[39m RandomPlayer(game)\u001b[38;5;241m.\u001b[39mplay\n\u001b[1;32m      8\u001b[0m coach \u001b[38;5;241m=\u001b[39m Coach(game)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OthelloGame' is not defined"
     ]
    }
   ],
   "source": [
    "# You may use this cell for debugging.\n",
    "# Remove/comment these codes before submitting the assignment to save run time.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "game = OthelloGame(5)  # An Othello game with a 5*5 board\n",
    "random_player = RandomPlayer(game).play\n",
    "coach = Coach(game)\n",
    "coach.train()\n",
    "print(\"\\nTESTING\")\n",
    "arena = Arena(coach.play, random_player, game)\n",
    "oneWon, twoWon, draws = arena.playGames(100)\n",
    "fraction_won = oneWon / 100\n",
    "print(\"Fractin won: \", fraction_won)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "hw_9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
